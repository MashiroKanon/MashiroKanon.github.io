<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>生成学习算法</title>
      <link href="/2022/08/10/%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
      <url>/2022/08/10/%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>Key words : 高斯判别分析(GDA)、朴素贝叶斯、拉普拉斯平滑、针对文本分类的事件模型</p><h2 id="综述">综述</h2><p>在上一讲中，我们的算法的逻辑是在所有样本中划分一条线，线的这一侧属于大象，另一侧属于小狗。当有要预测新的值时，就要看新动物的值落在了哪个划分区域中。这种算法也称为<strong>判别式算法</strong>。</p><p>换句话说，在训练集中，我们做的是$p(y|x;\theta)$，然后训练好$\theta$后对测试集求出p(y|x)。</p><p>在这一讲，我们分别观察大象和小狗，然后分别对他们进行建模，找到它们的特征。判断新动物属于哪一类时就要判断新动物更接近哪个训练集中已有的模型。这种算法被称为<strong>生成学习算法</strong>。</p><p>换句话说，我们在测试集中做的是p(x|y)，然后对测试集我们用贝叶斯公式来寻找$\mathop{argmax}\limits_y$。</p><h2 id="高斯判别分析">高斯判别分析</h2><h3 id="多元正态分布">多元正态分布</h3><p>n维多元正态分布，可以写成$N(\mu,\Sigma)$的分布形式，n维向量$\mu$是均值，$\Sigma$是协方差矩阵$\in R^{n\times n}$, 其密度函数为</p><p>$$<br>p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))<br>$$</p><p>假设随机变量Z是一个有值的向量，那么Z的协方差定义是$cov(Z)=E[(Z-E[Z])][Z-E(Z)^T]=E[ZZ^T]-(E[Z])(E[Z])^T$。</p><p>如果$X\ \sim\ N(\mu,\Sigma)$，则有$Cov(X)=\Sigma$</p><img width = '500' height = '150' src ="/pictures/机器学习/生成学习算法/PNGイメージ 1.png"><p>协方差矩阵$\Sigma$变大，高斯分布的形态就变得宽平，协方差矩阵$\Sigma$变小，分布就会变得集中。</p><img width = '500' height = '150' src ="/pictures/机器学习/生成学习算法/PNGイメージ 2.png"><img width = '500' height = '150' src ="/pictures/机器学习/生成学习算法/PNGイメージ 3.png"><p>调整均值$\mu$，就可以让密度图像移动</p><img width = '500' height = '150' src ="/pictures/机器学习/生成学习算法/PNGイメージ 4.png"><h3 id="1-2-高斯判别分析模型——X是连续的">1.2 高斯判别分析模型——X是连续的</h3><p>模型假设为</p><p>$y\ \sim\ Bernoulli(\phi)$</p><p>$x|y=0\ \sim\ N(\mu_0,\Sigma)$</p><p>$x|y=1\ \sim\ N(\mu_1,\Sigma)$</p><p>虽然这里有两个不同的均值向量$\mu$，但是针对这个模型，我们一般还是一种一个协方差矩阵$\Sigma$</p><p>取对数的似然函数(log-likelihood)为</p><p>$$<br>l(\phi,\mu_0,\mu_1,\Sigma)=log\prod_{i=1}^mp(x^{(i)},y^{(i)};\phi,µ_0,µ_1,\Sigma)= log\prod_{i=1}^mp(x^{(i)}|y^{(i)};µ_0,µ_1,\Sigma)p(y^{(i)};\phi)<br>$$</p><p>通过使l取得最大值，找到对应的参数组合</p><p>$\phi = \frac{1}{m}\sum_{i=1}^m1{y^{(i)}=1}$</p><p>$\mu_0=\frac{\sum_{i=1}^m1{y^{(i)}=0}x^{(i)}}{\sum_i=1^m1{y^{(i)}=0}}$</p><p>$\mu_0=\frac{\sum_{i=1}^m1{y^{(i)}=1}x^{(i)}}{\sum_i=1^m1{y^{(i)}=1}}$</p><p>$\Sigma=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T$</p><h3 id="1-3-高斯判别分析-GDA-和logistic回归的关系">1.3 高斯判别分析(GDA)和logistic回归的关系</h3><p>y是x的函数</p><p>$p(y=1|x;\phi,\mu_0,\mu_1,\Sigma)=\frac{1}{1+exp(-\theta^Tx)}$</p><p>这个形式是logistic的形式。</p><p>也就是说p(x|y)符合多元高斯分布时，p(y|x)符合logistic回归模型。</p><p>但是反过来却不成立，因为高斯判别分析拥有更强的假设条件和约束。比如p(x|y)符合泊松分布时，其p(y|x)也是logistic回归的。</p><p>从图形上来说：</p><p>高斯分布p(x|y) ⇒ logistic p(y|x) 表示两个椭圆总是能切分开</p><p>logistic p(y|x) ≠&gt; 高斯分布p(x|y) 这表示能切分开的不一定是两个椭圆</p><p>我们在实际过程中往往事先不知道训练数据满足什么分布，不能做很强的假设，因此logistic回归的方法更为常用。</p><h2 id="2-朴素贝叶斯法-Naive-Bayes-——X是离散的">2 朴素贝叶斯法(Naive Bayes)——X是离散的</h2><p>在垃圾邮件过滤这个样例中，我们并不需要便利整个英文词典来组成所有英语单词的列表，实践中更常用的方法是遍历一下训练集，把出现过一次以上的单词编码成特征向量。让i表示字典中的第i个词，x_i表示这个词是否出现在邮件中。x的长度就是整个字典的长度。</p><img width = '500' height = '150' src ="/pictures/机器学习/生成学习算法/PNGイメージ 5.png"><p>接下来给p(x|y)建模，我们要假设特征向量$x_i$对于给定的y是独立的。注意这里的独立是条件独立，比如$p(x_{2087}|y)=p(x_{2087}|y,x_{39831})$。如果x不是条件独立的，那么这个例子就变成了p(x_2087)=p(x_2087|x_39831)。</p><p>由此我们得到等式</p><p>$p(x_1,…,x_{50000}|y){=p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2)\dots p(x_{50000}|y,x_1,x_2,…,x_{50000})}$<br>${=p(x_1|y)p(x_2|y)\dots p(x_{50000}|y)}$<br>${=\prod_{i=1}^np(x_n|y)}$</p><p>根据模型的参数$\phi_{i|y=1}=p(x_i=1|y=1)$，$\phi_{i|y=0}=p(x_i=1|y=0)$，$\phi_y=p(y=1)$，写出联合似然函数</p><p>$$<br>l(\phi_y,\phi_{i|y=0},\phi_{i|y=1})=\prod_{i=1}^mp(x^{(i)},y^{(i)})<br>$$</p><p>求解得：</p><p>$\phi_{j|y=1}=\frac{\sum_{i=1}^m1{x_j^{(i)}\lor y^{(i)}=1}}{\sum_{i=1}^m1{y^{(i)}=1}}$</p><p>$\phi_{j|y=0}=\frac{\sum_{i=1}^m1{x_j^{(i)}\lor y^{(i)}=0}}{\sum_{i=1}^m1{y^{(i)}=0}}$</p><p>$\phi_y=\frac{\sum_{i=1}^m1{y^{(i)}=1}}{m}$</p><p>第一个和第二个式子表示y=1或y=0的样本中，x_j=1的比例，第三个式子表示y=1的样本占总样本的比例。</p><p>当测试新样本时</p><p>$<br>{p(y=1|x)=\frac{p(x|y=1)p(y=1)}{p(x)}}$<br>${=\frac{\prod_{i=1}^np(x_i|y=1)p(y=1)}{\prod_{i=1}^np(x_i|y=1)p(y=1)+ \prod_{i=1}^np(x_i|y=0)p(y=0)}}$</p><h2 id="3-拉普拉斯平滑">3 拉普拉斯平滑</h2><p>针对朴素贝叶斯方法，其致命缺点时对数据稀疏问题过于敏感。如果某一个词在训练数据中从未出现过，那么我们在计算概率时，p(y=1|x)=0。为了解决这个问题，我们打算给未出现的特征值赋予一个小的值而非0。</p><p>假设问题是估计一个多项式随机变量z，z的纬度为k，给定m个独立观测组成的集合，最大似然估计的形式为</p><p>$\phi_j = \frac{\sum_{i=1}^m1{z^{(i)}=j}}{m}$</p><p>因此，我们引入拉普拉斯平滑，将上面的估计替换成</p><p>$\phi_j = \frac{\sum_{i=1}^m1{z^{(i)}=j}+1}{m+k}$</p><p>使用了拉普拉斯光滑之后，上面的朴素贝叶斯分选器的例子对参数的估计就写成了</p><p>$\phi_{j|y=1}=\frac{\sum_{i=1}^m1{x_j^{(i)}\lor y^{(i)}=1}+1}{\sum_{i=1}^m1{y^{(i)}=1}+2}$</p><p>$\phi_{j|y=0}=\frac{\sum_{i=1}^m1{x_j^{(i)}\lor y^{(i)}=0}+1}{\sum_{i=1}^m1{y^{(i)}=0}+2}$</p><p>在实际应用中，$\phi_y$是否使用拉普拉斯平滑平没有太大的影响。</p><h2 id="4-针对文本分类的事件模型">4 针对文本分类的事件模型</h2><p>上一节中朴素贝叶斯算法是从词典入手，首先建立一个词典，然后将出现的词标记为1，否则标0。让i表示词典中的第i个词，$x_i$表示这个词是否出现在邮件中。</p><p>如果我们从邮件入手，让i表示邮件中的第i个词，$x_i$表示这个词在字典中的位置。x的长度n是可变的，因为每封邮件的词的数量不一定相同。这样文本分类模型是一个多项式事件模型，$x_i|y$现在是一个多项式分布，而不是伯努利分布了。</p><p>$<br>{l(\phi_y,\phi_{i|y=0},\phi_{i|y=1})=\prod_{i=1}^mp(x^{(i)},y^{(i)})<br>}$<br>${=\prod_{i=1}^m(\prod_{j=1}^{n_i}p(x_j^{(i)}|y;\phi_{i|y=0},\phi_{i|y=1}))p(y^{(i)};\phi_y)}$</p><p>解得</p><p>$\phi_{k|y=1}=\frac{\sum_{i=1}^{n_i}1{x_j^{(i)}=k \lor y^{(i)}=1}}{\sum_{i=1}^m1{y^{(i)}=1}n_i}$</p><p>$\phi_{k|y=0}=\frac{\sum_{i=1}^{n_i}1{x_j^{(i)}=k \lor y^{(i)}=0}}{\sum_{i=1}^m1{y^{(i)}=0}n_i}$</p><p>$\phi_y=\frac{\sum_{i=1}^m1{y^{(i)}=1}}{m}$</p><p>与之前的式子相比，分母多了个$n_i$，分子由0/1变成了k。</p><p>从直觉来讲，</p><p>$\phi_{k|y=1}=\frac{所有y=1的邮件中位置是k的单词的个数之和}{所有y=1的邮件的词数之和}$，与朴素贝叶斯算法的区别来源于特质值的维度不再是相同的。</p><p>如果使用拉普拉斯平滑，公式变为：</p><p>$\phi_{k|y=1}=\frac{\sum_{i=1}^{n_i}1{x_j^{(i)}=k \lor y^{(i)}=1}+1}{\sum_{i=1}^m1{y^{(i)}=1}n_i+|V|}$</p><p>$\phi_{k|y=0}=\frac{\sum_{i=1}^{n_i}1{x_j^{(i)}=k \lor y^{(i)}=0}+1}{\sum_{i=1}^m1{y^{(i)}=0}n_i+|V|}$ , |V|是词典的个数。</p><p>一个文本分类事件模型(不带拉普拉斯平滑)的例子</p><img width = '500' height = '350' src ="/pictures/机器学习/生成学习算法/PNGイメージ 6.png"><img width = '500' height = '150' src ="/pictures/机器学习/生成学习算法/PNGイメージ 7.png">]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对回归方法的认识</title>
      <link href="/2022/08/05/%E5%AF%B9%E5%9B%9E%E5%BD%92%E6%96%B9%E6%B3%95%E7%9A%84%E8%AE%A4%E8%AF%86/"/>
      <url>/2022/08/05/%E5%AF%B9%E5%9B%9E%E5%BD%92%E6%96%B9%E6%B3%95%E7%9A%84%E8%AE%A4%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<h2 id="监督学习-Supervise-learning">监督学习(Supervise learning)</h2><p>符号定义：</p><p><strong>输入特征：$x^{(i)}$</strong>，比如房屋面积，</p><p><strong>目标变量：</strong>$y^{(i)}$，比如房屋价格，</p><p><strong>训练样本：</strong>$(x^{(i)},y^{(i)})$</p><p><strong>训练集</strong>：让机器来学习的数据集，就是一个长度为m的训练样本的列表：${(x^{(i)},y^{(i)});i=1,…,m}$</p><p>上标(i)在这里只作为训练集的索引记号。</p><p><strong>假设(hypothesis)</strong>：即函数h，X→Y，h(x)是一个对应的真实的y值比较接近的评估值。</p><p>如果我们要预测的目标变量Y是连续的，这种学习问题就被称为回归问题；</p><p>我们要预测的Y只能取一小部分的离散的值，这类问题被称为分类问题。</p><h2 id="线性回归">线性回归</h2><p>我们给输入特征增加维度的概念，$x_1^{(i)}$表示训练集中第一个特征的第i个训练样本的值。</p><p>现在考虑一个二维输入特征的情况：</p><p>$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2$</p><p>$\theta_i$是<strong>参数</strong>，或叫<strong>权重</strong>，是从X到Y的线性函数映射的空间参数。</p><p>设$x_0=1$，这样我们就有</p><p>$h(x)=\sum_{i=0}^n\theta_ix_i=\theta^Tx$</p><p>这里的n表示的不是样本的长度，而是样本输入特征的维度。</p><h3 id="成本函数">成本函数</h3><p>有了训练集，有了假设，那么如何挑选参数使得h(x)尽可能逼近y呢？</p><p>我们就需要定义一个成本函数(cost function)、损失函数(loss function)</p><p>$J(\theta)=\frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$</p><p>目的是$\min\limits_\theta\ J_{\theta}$</p><p>J函数有两个要注意的点：1/2和差值平方。</p><p>1/2是为了在求导时抹掉系数。</p><h2 id="最小均方算法">最小均方算法</h2><h3 id="梯度下降法">梯度下降法</h3><p>梯度下降法是求极小值问题的一个解法，但是梯度下降法最大的问题是求得的有可能是局部最小值，这与初始点的选取有关。</p><p>梯度下降法的流程是：</p><ol><li>首先对$\theta$赋值，这个值可以是随机的，也可以是一个全零的向量，</li><li>改变$\theta$的值，使得$J(\theta)$按梯度下降的方向进行减少。</li></ol><p>$\theta_j = \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)$</p><p>损失函数中的$\theta$是完整维度的，$J(\theta)$是使得所有维度，所有样本量的差值最小的一个函数。但是在更新中的$\theta$，却是每次只能更新一个维度。所以我们这个方程重复n次，即x的每个维度都要照顾到。</p><p>$\alpha$在这里被称为<strong>学习速率</strong>。</p><p>考虑只有一个样本(x,y)的情况，这样我们不需要在考虑损失函数中的m个样本量的求和。</p><p>$$<br>\frac{\partial}{\partial\theta_j}J(\theta)=\frac{\partial}{\partial\theta_j}\frac{1}{2}(h_\theta(x)-y)^2\=(h_\theta(x)-y)x_j<br>$$</p><p>$\theta_j = \theta_j +\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$</p><p>这个结果也称为LMS更新规则(least mean squares)，即最小均方。</p><p>每次更新的大小与y-h的差额成正比，因此如果预测值与y相差不大，就不需要更改参数了，但反之，如果有很大误差，则需要更改参数。</p><h4 id="批量梯度下降法">批量梯度下降法</h4><p>在每一个步长内检查所有整个训练集中的所有样本。</p><p>当训练集的训练样本有m个时 ,重复直到收敛{</p><p>$\theta_j = \theta_j +\alpha\sum^m_{i=1}(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}\ \ \ \ \  for \ every\ j$</p><p>}</p><p>J是一个凸的二次函数，下面是一个梯度下降的例子，</p><img width = '500' height = '180' src ="/pictures/机器学习/对回归方法的认识/梯度下降.png"><h4 id="随机梯度下降法">随机梯度下降法</h4><p>Loop {</p><p>for i=1 to m, {</p><p>$\theta_j = \theta_j + \alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}\ \ \ \ \ \ \ for\ every\ j$</p><p>}</p><p>}</p><p>在这里，每次遇到一个训练样本，都会根据单一训练样本的误差梯度来对参数进行更新，这种算法叫随机梯度下降法。</p><p>批量梯度下降法要在运行第一步之前对整个训练集进行扫描便利，所以当训练集的规模m很大时，会带来较大的性能开销。</p><p>而随机梯度下降法则没有这个缺点，而是可以立即开始。</p><p>所以更推荐使用随机梯度下降法。</p><h2 id="法方程">法方程</h2><p>另一种找到J函数最小值的方法是直接进行矩阵运算，求出对应导数为0时的$\theta_j$。</p><h2 id="最小二乘法-Least-squares">最小二乘法(Least squares)</h2><p>给定一个训练集，<strong>设计矩阵(design matrix)X</strong>设置为一个m<em>n矩阵（如果考虑到截距项，就是m</em>(n+1)），m是样本集的长度，n是样本特征的个数。</p><p>法线方程(normal equations)</p><p>$X^TX\theta = X^T\vec y$</p><p>所以让J取最小值的$\theta$就是$\theta=(X^TX)^{-1}X^T\vec y$</p><p>此方法要求X是列满秩的。</p><h2 id="概率解释">概率解释</h2><p>首先假设目标变量和输入值存在以下关系</p><p>$$<br>y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)} \tag 1<br>$$</p><p>在这里\epsilon是误差项，并进一步假设其是独立同分布的且服从于均值为0，方差为\sigma^2的高斯分布，这样\epsilon的密度函数为</p><p>$$<br>f(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2}) \tag2<br>$$</p><p>将方程1等式带入方程2，我们可以得到</p><p>$Y^{(i)}|x^{(i)};\theta\ \sim\ N(\theta^Tx^{(i)},\sigma^2)$</p><p>我们把它看作是\theta的函数时，可以称他为似然函数</p><p>$L(\theta)=L(\theta;X,\vec y)=p(\vec y)$</p><p>最大似然法告诉我们的是要选择能让数据的似然函数尽可能最大的$\theta$。</p><p>通过推导，</p><p>$l(\theta)=log\ L(\theta)=m\ log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{sigma^2}\cdot \frac{1}{2}\sum^m_{i=1}(y^{(i)-\theta^Tx^{(i)}})^2$</p><p>因此，对$l(\theta)$的最大值也意味着$\frac{1}{2}\sum^m_{i=1}(y^{(i)-\theta^Tx^{(i)}})^2$，这个式子就是$J(\theta)$</p><p>总结一下，在对数据进行概率假设的基础上，最小二乘回归得到的\theta和最大似然估计法得到的\theta是一致的。</p><h2 id="局部加权线性回归">局部加权线性回归</h2><img width = '500' height = '150' src ="/pictures/机器学习/对回归方法的认识/欠拟合与过拟合.png"><p>第一张图是一个**欠拟合(under fitting)**的例子,</p><p>第三张图是一个**过拟合(over fitting)**的例子。</p><p>这就需要用到局部加权线性回归(LWR)的方法，这个方法是假设有足够多的训练数据，对不太重要的特征进行一些筛选。</p><p>在LWR中</p><ol><li>使用参数$\theta$进行拟合，让加权距离$w^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$最小，</li><li>输出$\theta^Tx$。</li></ol><p>$w^{(i)}$是非负的权值，如果对应某个i的权值$w^{(i)}$特别大，那么选择拟合参数$\theta$时，就要尽量让$(y^{(i)}-\theta^Tx^{(i)})^2$小一些；如果权值w^特别小，那么对应的$(y^{(i)}-\theta^Tx^{(i)})^2$就基本在拟合过程中忽略掉了。</p><p>关于权值的选择可以用</p><p>$w^{(i)}=exp(-\frac{(x^{(i)}-x)^2}{2r^2})$</p><p>如果$|x^{(i)}-x|$非常小，那么权值就接近1；如果$|x^{(i)}-x|$非常大，那么权值就会变小。换句话说，$\theta$的选择过程中，查询点x附近的训练样本有更高的权值。</p><p>参数\tau控制了参数降低的速度，也叫<strong>带宽参数。</strong></p><p>无权重的线性回归算法是一种参数学习算法，</p><p>有权重的线性回归算法是一种非参数算法。</p><h2 id="逻辑回归">逻辑回归</h2><p>确定$y\in{0,1}$，就是说y必然应当是0和1的一个，所以就可以改变一下假设函数$h_\theta(x)$的形式来解决这个问题</p><p>$h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$</p><p>g(z)叫做逻辑函数(logistic function)，或者叫双弯曲S型函数(sigmoid function)。</p><img width = '500' height = '200' src ="/pictures/机器学习/对回归方法的认识/logistic model.png"><p>$g’(z)=g(z)(1-g(z))$</p><p>假设</p><p>$p(y=1|x;\theta)=h_\theta(x)$</p><p>$p(y=0|x;\theta)=1-h_\theta(x)$</p><p>或者说</p><p>$P(y|x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^(1-y)$</p><p>假设m个训练样本是各自独立生成的，那么</p><p>$L(\theta)=\prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^(1-y^{(i)})$</p><p>如何要让似然函数最大呢？这里采用<strong>梯度上升法</strong>，对似然函数求导得到$\frac{\partial}{\partial \theta_j}l(\theta) = (y-h_\theta(x))x_j$</p><p>根据我们之前讨论的，我们要使似然函数最大，每一步的大小由导数决定，因此我们得到了随机梯度上升的规则</p><p>$\theta_j = \theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$</p><p>这与LMS并不是同一个算法，因为这里的假设函数h是一个非线性函数。</p><h2 id="感知器学习算法">感知器学习算法</h2><p>与逻辑回归相比，这里改变了g的定义，使其成为一个阈值函数(threshold function)。</p><p>$g(z)=1\ \ if \ z\geq0$</p><p>$\ \ \ \ =0\ if\ z&lt;0$</p><p>更新规则为</p><p>$\theta_j = \theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$</p><h2 id="牛顿法解最大似然估计">牛顿法解最大似然估计</h2><p>仍然假设S型函数g(z)来进行逻辑回归的情况。</p><p><strong>牛顿法</strong>是当我们要找$f(\theta)=0$时，我们就对$\theta$作如下更新</p><p>$\theta = \theta-\frac{f(\theta)}{f’(\theta)}$</p><p>图解为<br><img width = '500' height = '150' src ="/pictures/机器学习/对回归方法的认识/牛顿法.png"></p><p>斜率$f’(\theta)=\frac{f(\theta)}{\Delta}$</p><p>从牛顿法我们得到启发，如果我们想找似然函数的最大值，就要找似然函数的导数为0的时候的$\theta$值，于是我们得到标量的更新规则</p><p>$\theta = \theta-\frac{l’(\theta)}{l’’(\theta)}$</p><p>下面我们扩展到向量形式，也叫<strong>牛顿-拉普森法</strong></p><p>$\theta = \theta - H^{-1}\nabla_\theta l(\theta)$</p><p>$\nabla_\theta l(\theta)$是$l(\theta)$对偏导数向量，而H是一个n<em>n的矩阵（如果包括截距项的话，应该是(n+1)</em>(n+1)）</p><p>$H_{ij}=\frac{\partial^2l(\theta)}{\partial \theta_i \partial \theta_j}$</p><p>牛顿法通常比（批量）梯度下降法收敛得更快，而且达到最小值所需要的迭代次数也少很多。但是牛顿法的单次迭代中往往要比梯度下降法的单步消耗更多的性能。 但只要n不是很大，牛顿法还是有优势的。</p><h2 id="广义线性模型">广义线性模型</h2><h3 id="指数族">指数族</h3><p>如果一个分布能用下面的方式来写出来，我们就说这类分布属于指数族</p><p>$p(y;\eta)=b(y)exp(\eta^TT(y)-a(\eta))$</p><p>$\eta$叫做次分布的自然参数，</p><p>T(y)叫做充分统计量，它与$\eta$具有同一个维度，</p><p>b(y)是一个基础特征(base measure)，</p><p>a($\eta$)是一个对数分割函数。</p><p>指数族满足以下特征，</p><ol><li><p>MLE with respect to $\eta$ is concave</p><p>NLL(negative log-likelihood) is convex</p></li><li><p>$E[y;\eta]=\frac{\partial}{\partial \eta}a(\eta)$</p></li><li><p>$Var[y;\eta]=\frac{\partial^2}{\partial \eta^2}a(\eta)$</p></li></ol><p>我们之前的函数：伯努利分布和高斯分布均属于指数分布族，除此之外，多项式分布，泊松分布，$\gamma$和指数分布，$\beta$和狄利克雷分布都属于指数分布族。</p><aside>💡 如何恰当选择分布？</aside><p>通常情况下，实际值数据使用高斯分布；二进制数据，使用伯努利分布；如果计数非负整数，则使用泊松分布；如果是正实数，使用$\Gamma$或指数分布；如果是概率分布，则使用\beta和狄利克雷分布，这两个主要出现在贝叶斯机器学习和贝叶斯统计中。</p><h3 id="构建广义线性模型">构建广义线性模型</h3><p>三个假设</p><ol><li>$y|x;\theta\ \sim\ Exponential \ Family(\eta)$，即给定x和\theta，y的分布属于指数分布族，是一个参数为$\eta$的指数分布。</li><li>给定x，目的是预测的假设h的值要尽可能接近给定x的T(y)的期望值。大多数例子中T(y)=y，所以要满足$h_\theta(x)=E[y|x;\theta]$</li><li>自然参数\eta和输入值x是线性相关的，$\eta=\theta^Tx$。</li></ol><img width = '500' height = '200' src ="/pictures/机器学习/对回归方法的认识/广义线性模型.png"><p>Learning update rule.</p><p>$$<br>\theta_j = \theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}<br>$$</p><p>根据指数族的特征2，$\mu=E[y;\eta]=g(\eta)=\frac{\partial}{\partial \eta}a(\eta)$ ，这个式子有一个名字叫<strong>规范响应函数</strong>。它的反函数，$\eta=g^{-1}(\mu)$，被称作<strong>规范链接函数</strong>。</p><p>回顾一下，我们一共有三种参数：</p><p>一种是模型参数：$\theta$，这是我们训练的</p><p>一种是自然参数：$\eta$，通过\theta^Tx，自然参数由模型参数线性给出，这就是我们做的设计选择。</p><p>还有一种是规范参数：$\phi$ - 伯努利分布，$\mu\sigma^2$ - 高斯分布，$\lambda$ - 泊松分布，规范参数通过规范响应函数g和规范链接函数g^{-1}与自然参数互相给出。</p><p>我的理解是，每一个$\theta$都会形成自己的分割超平面，然后对于横轴上的每一个x，都会形成自己的分布方式，这是一个理想化的情况。然后第二张图是实际的样本的分布情况，将第一张图的理想化的分布图拿过来，看这些样本的拟合情况，对$\theta$进行修正并找到最好的那个$\theta$。</p><p>Sigmoid模型是把所有的点都压缩在(0,1)之间了，然后再根据训练样本的点的分布来修正$\theta$。</p><p>注意，不管是高斯分布也好，还是Sigmoid也好，第一幅图和第三幅图它们都只有一套坐标轴。</p><p>说白了其实还是上面那个流程图，只不过图形化了而已。</p><h3 id="Softmax回归">Softmax回归</h3><p>如果相应变量y不是二元的，而是多元的，那么我们就要用到多项式分布。</p><p>此时的T(y)不再是y，而是一个具有k-1维的向量，k是y的输出值的种类数。</p><img width = '700' height = '150' src ="/pictures/机器学习/对回归方法的认识/softmax结果.png"><p>这时，有k个不同$\theta$分属于不同的类。我们之前有提到，sigmoid的二元选择只有1（=2-1）个$\theta$，那么为什么多元情况下就有k个不同类了呢？</p><p>一个$\theta$相当于一个分割的线。如果我们想将三个类分开，那么需要三条线才可以完全将他们分开。</p><img width = '500' height = '350' src ="/pictures/机器学习/对回归方法的认识/softmax回归.png"><p>假设函数输出所有类的概率的分布情况，就是第三张图。</p><p>我们的目的是让第三张图与第四张图尽可能接近，也就是使两个分布之间的交叉熵最小。</p><p>$$<br>Cross \ Ent(p,\hat p)=-\sum_{y\in class}p(y)log\hat p(y)=-log\hat p(y_\triangle)=-log\frac{e^{\theta_\triangle^Tx}}{\sum_{i\in class}e^{\theta_i^Tx}}<br>$$</p><p>我们进行样本的训练并进行梯度下降以获得更小的熵。</p>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/08/05/hello-world/"/>
      <url>/2022/08/05/hello-world/</url>
      
        <content type="html"><![CDATA[<svg class="icon" style="width:4em; height:4em" aria-hidden="true"><use xlink:href="#icon-mianbao"></use></svg><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start">Quick Start</h2><h3 id="Create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
